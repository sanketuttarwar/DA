{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nb:\n",
    "    def fit_nb(self, x, y):\n",
    "        #### note x is a 2d array hence first dimension is no_of tuples and other dimension is no_of features\n",
    "        #### class variable _classes hold uniques class labels of dataset passed\n",
    "        ##### no_of_tuples holds no_of_rows passed in dataset\n",
    "        no_of_tuples,no_of_features = x.shape\n",
    "        self._classes = np.unique(y)\n",
    "        no_of_classes = len(self._classes)\n",
    "        \n",
    "        ### a 2d array class variable _mean will hold mean values for each feature for respective class in given format \n",
    "        #### class mean_of_feature_x1 mean_of_fetaure_x2 ....   ....  .....\n",
    "        #### 0      m1                m2                 ....   ....  .....\n",
    "        #### _priors is a vector to hold prior probablity of each class(calculated as no_of_tuples_of_resp_class/total_tuples)\n",
    "        self._mean = np.zeros((no_of_classes,no_of_features),dtype = np.float64)\n",
    "        self._var = np.zeros((no_of_classes,no_of_features),dtype = np.float64)\n",
    "        self._priors = np.zeros(no_of_classes,dtype = np.float64)\n",
    "        \n",
    "        ##### evaluating mean and variance and prior probs feature values of all features for each unique class\n",
    "        for c in self._classes:\n",
    "            ###x_c represents those samples which have class label as c\n",
    "            x_c = x[y==c]\n",
    "            self._mean[c,:] = x_c.mean(axis = 0)\n",
    "            self._var[c,:] = x_c.var(axis = 0)\n",
    "            self._priors[c] = x_c.shape[0] / float(no_of_tuples) \n",
    "    \n",
    "    #### a function to evaluate classes for the passed 2d testing array\n",
    "    def pred_nb(self, x_test):\n",
    "        y_pred = [self.pred_for_one_sample(one_sample) for one_sample in x_test] ################################\n",
    "        return y_pred\n",
    "    \n",
    "    ######NOTE: since our only aim is to evaluate class label hence we will evaluate class by using direct proportionality \n",
    "    ##i.e neglect the denominator while evaluating posterior since it is constant for all and we will be evaluating \n",
    "    ### log(numerator)  numerator  =  p(x1/y)*p(x2/y)*...*p(xk/y)*p(y) (neglecting denomi as it is constant) hence \n",
    "    ### log(nume) = log(p(x1/y)) + p(x2/y) + p(x3/y) + .... + log(p(y))\n",
    "    \n",
    "    ##### a helper module to evaluate class label for each tuple \n",
    "    def pred_for_one_sample(self, sample_vector):\n",
    "        posteriors = []\n",
    "        #### assigning 0,1,2,3,.... for class c1,c2,c3,c4.....\n",
    "        for id_x,c in enumerate(self._classes):\n",
    "            #### calculate p(y) or say prior of particular class\n",
    "            prior = np.log(self._priors[id_x])\n",
    "            sum_of_conditional_prob_for_each_feature = np.sum(np.log(self._pdf(id_x,sample_vector)))\n",
    "            posterior = prior + sum_of_conditional_prob_for_each_feature\n",
    "            posteriors.append(posterior)\n",
    "            \n",
    "        #### return class label of that class which has maximum p(y/x) or maximum posterior value\n",
    "        return self._classes[np.argmax(posteriors)]   ################################\n",
    "                \n",
    "    ### probablity density func\n",
    "    def _pdf(self, class_idx, vect):\n",
    "        #### mean vector(vector contains mean of all features) for that particular class \n",
    "        mean1 = self._mean[class_idx]\n",
    "        var1 = self._var[class_idx]\n",
    "        #diff = np.subtract(vect,mean1)\n",
    "        numerator = np.exp(-1 * np.square(vect-mean1) / (2*var1))\n",
    "        #numerator = np.exp(np.square(diff) / (2*var1))\n",
    "        denominator = np.sqrt(2* np.pi *var1)\n",
    "        #print(vect)\n",
    "        return numerator/denominator\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### self writtten  functionn for accuracy\n",
    "def acc(y_ac,y_pr):\n",
    "    temp = np.sum(y_ac==y_pr)\n",
    "    return temp/len(y_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = pd.read_csv('diabetes.csv', header=None)\n",
    "data.head()\n",
    "\n",
    "y1 = data.iloc[:,-1]\n",
    "x1 = data.iloc[:,0:8]\n",
    "\n",
    "x_train1,x_test1,y_train1,y_test1 = train_test_split(x,y,test_size=0.2,random_state=123)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "naiveb = nb()\n",
    "naiveb.fit_nb(x_train1,y_train1)\n",
    "y_pr1 = naiveb.pred_nb(x_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.965\n"
     ]
    }
   ],
   "source": [
    "print(acc(y_test1,y_pr1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
